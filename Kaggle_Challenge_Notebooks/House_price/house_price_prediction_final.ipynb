{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Various Imports '''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import RidgeCV,LassoCV,ElasticNetCV\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Importing train and test data sets '''\n",
    "house_train = pd.read_csv(\"train.csv\")\n",
    "house_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Combining train and test data sets for data processing steps '''\n",
    "house_data = pd.concat((house_train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                       house_test.loc[:,'MSSubClass':'SaleCondition']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(house_data.shape)\n",
    "house_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Performing basic EDA steps'''\n",
    "# print(house_data.describe())\n",
    "# print(house_data.info())\n",
    "print(house_data.dtypes[house_data.dtypes == 'object'].count())\n",
    "print(house_data.dtypes[house_data.dtypes != 'object'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [house_data[c].value_counts() for c in list(house_data.select_dtypes(include='object').columns)]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Since there are lot of categorical features which are ordinal in nature, so we will encode them in proper order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Also Substituting missing values for few columns whose missing values have a predefined value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data['Street'] = house_data['Street'].map({'Grvl':1,'Pave':2})\n",
    "house_data['Alley'] = house_data['Alley'].map({'Grvl':1,'Pave':2,np.nan:0})\n",
    "house_data['LotShape'] = house_data['LotShape'].map({'Reg':3,'IR1':2,'IR2':1,'IR3':0})\n",
    "house_data['Utilities'] = house_data['Utilities'].map({'AllPub':4,'NoSewr':3,'NoSeWa':2,'ELO':1})\n",
    "house_data['LandSlope'] = house_data['LandSlope'].map({'Gtl':3,'Mod':2,'Sev':1})\n",
    "house_data['ExterQual'] = house_data['ExterQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1})\n",
    "house_data['ExterCond'] = house_data['ExterCond'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1})\n",
    "house_data['BsmtQual'] = house_data['BsmtQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,np.nan:0})\n",
    "house_data['BsmtCond'] = house_data['BsmtCond'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,np.nan:0})\n",
    "house_data['BsmtExposure'] = house_data['BsmtExposure'].map({'Gd':5,'Av':4,'Mn':3,'No':2,np.nan:1})\n",
    "house_data['BsmtFinType1'] = house_data['BsmtFinType1'].map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,np.nan:0})\n",
    "house_data['BsmtFinType2'] = house_data['BsmtFinType2'].map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,np.nan:0})\n",
    "house_data['HeatingQC'] = house_data['HeatingQC'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1})\n",
    "house_data['CentralAir'] = house_data['CentralAir'].map({'Y':1,'N':0})\n",
    "house_data['KitchenQual'] = house_data['KitchenQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1})\n",
    "house_data['FireplaceQu'] = house_data['FireplaceQu'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,np.nan:0})\n",
    "house_data['GarageFinish'] = house_data['GarageFinish'].map({'Fin':3,'RFn':2,'Unf':1,np.nan:0})\n",
    "house_data['GarageQual'] = house_data['GarageQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,np.nan:0})\n",
    "house_data['GarageCond'] = house_data['GarageCond'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,np.nan:0})\n",
    "house_data['PavedDrive'] = house_data['PavedDrive'].map({'Y':3,'P':2,'N':1})\n",
    "house_data['PoolQC'] = house_data['PoolQC'].map({'Ex':4,'Gd':3,'TA':2,'Fa':1,np.nan:0})\n",
    "house_data['Fence'] = house_data['Fence'].map({'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1,np.nan:0})\n",
    "house_data['YrSold'] = house_data['YrSold'].map({2010:5,2009:4,2008:3,2007:2,2006:1})\n",
    "# house_data['MSSubClass'] = house_data['MSSubClass'].map({190:16,180:15,160:14,150:13,120:12,90:11,85:10,80:9,75:8,70:7,60:6,50:5,45:4,40:3,30:2,20:1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data['MiscFeature'] =house_data['MiscFeature'].fillna('None')\n",
    "house_data['MSSubClass'] = house_data['MSSubClass'].astype('object')\n",
    "house_data['LotFrontage'] =house_data['LotFrontage'].fillna(house_data['LotFrontage'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(house_data.dtypes[house_data.dtypes == 'object'].count())\n",
    "print(house_data.dtypes[house_data.dtypes != 'object'].count())\n",
    "house_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Checking for null values now after substitution '''\n",
    "print(house_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Identifying features with missing values above 15% for removal '''\n",
    "\n",
    "drop_features = []\n",
    "\n",
    "nulls_df = pd.DataFrame((100*house_data.isnull().sum()/len(house_data)).sort_values(ascending=False).apply(lambda x: int(float(\"{0:.2f}\".format(x))))).reset_index() \n",
    "nulls_df = nulls_df.rename(columns= {'index':'column_name',0:'value'})\n",
    "#print(nulls_df.columns)\n",
    "drop_features.extend(nulls_df[nulls_df.value > 15].column_name.tolist())\n",
    "print(drop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nulls_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Identifying too skewed features (numerical) '''\n",
    "numeric_feats = house_data.dtypes[house_data.dtypes != \"object\"].index\n",
    "sk_df = pd.DataFrame({'skewness': house_data[numeric_feats].apply(lambda x: stats.skew(x.dropna()))})\n",
    "sk_df = sk_df.sort_values('skewness',ascending=False)\n",
    "sk_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' BOX-COX transformation of Skewed Features'''\n",
    "sk_df = sk_df[abs(sk_df) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(sk_df.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = sk_df.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    house_data[feat] = boxcox1p(house_data[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Identifying Highly correlated features to remove '''\n",
    "corr_remove = []\n",
    "cols = house_data.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(10,10))\n",
    "co_cols = cols[:10]\n",
    "co_cols.append('SalePrice')\n",
    "sns.heatmap(house_data[co_cols].corr(), cmap='RdBu_r', annot=True,center=0.0)\n",
    "plt.title('Correlation between 1 ~ 10th columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "co_cols = cols[10:20]\n",
    "co_cols.append('SalePrice')\n",
    "sns.heatmap(house_data[co_cols].corr(), cmap='RdBu_r', annot=True,center=0.0)\n",
    "plt.title('Correlation between 10 ~ 20th columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "co_cols = cols[30:40]\n",
    "co_cols.append('SalePrice')\n",
    "sns.heatmap(house_data[co_cols].corr(), cmap='RdBu_r', annot=True,center=0.0)\n",
    "plt.title('Correlation between 30 ~ 40th columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "co_cols = cols[20:30]\n",
    "co_cols.append('SalePrice')\n",
    "sns.heatmap(house_data[co_cols].corr(), cmap='RdBu_r', annot=True,center=0.0)\n",
    "plt.title('Correlation between 20 ~ 30th columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "co_cols = cols[40:50]\n",
    "co_cols.append('SalePrice')\n",
    "sns.heatmap(house_data[co_cols].corr(), cmap='RdBu_r', annot=True,center=0.0)\n",
    "plt.title('Correlation between 40 ~ 50th columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "co_cols = cols[50:60]\n",
    "co_cols.append('SalePrice')\n",
    "sns.heatmap(house_data[co_cols].corr(), cmap='RdBu_r', annot=True,center=0.0)\n",
    "plt.title('Correlation between 50 ~ 60th columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "co_cols = cols[60:70]\n",
    "co_cols.append('SalePrice')\n",
    "sns.heatmap(house_data[co_cols].corr(), cmap='RdBu_r', annot=True,center=0.0)\n",
    "plt.title('Correlation between 60 ~ 70th columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "co_cols = cols[70:80]\n",
    "co_cols.append('SalePrice')\n",
    "sns.heatmap(house_data[co_cols].corr(), cmap='RdBu_r', annot=True,center=0.0)\n",
    "plt.title('Correlation between 70 ~ 80th columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = house_data.corr()\n",
    "high_corr = (corr >= 0.80).astype('uint8')\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(high_corr, cmap='RdBu_r', annot=True, center=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Adding more features which have collinearity to our drop_features list '''\n",
    "corr_remove.extend(['TotalBsmtSF','GarageCars','TotRmsAbvGrd','GarageYrBlt','Fireplaces','GarageQual','PoolQC','BsmtQual','BsmtFinSF1','BsmtFinSF2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = drop_features + corr_remove\n",
    "print(drop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Normalizing target vector '''\n",
    "house_train['SalePrice'] = np.log1p(house_train['SalePrice'])\n",
    "sns.distplot(house_train['SalePrice'],fit= stats.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' creating a new data frame without unnecessary features '''\n",
    "train_data = house_data.drop(columns=drop_features,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.fillna(inplace=True)\n",
    "print(train_data.isna().sum().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dummy = pd.get_dummies(train_data,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dummy.fillna(train_data_dummy.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing those features which contains almost 100% zero values in them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse = []\n",
    "\n",
    "for feature in train_data_dummy.columns:\n",
    "    counts = train_data_dummy[feature].value_counts()\n",
    "    zeros = counts.iloc[0]\n",
    "    if zeros / len(train_data_dummy) * 100 > 99.94:\n",
    "        sparse.append(feature)\n",
    "        \n",
    "train_data_dummy.drop(columns=sparse, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_dummy = np.log1p(train_data_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_dummy.shape)\n",
    "train_data_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_dummy.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating matrices for sklearn:\n",
    "x_train = train_data_dummy.iloc[:house_train.shape[0]].values\n",
    "x_test = train_data_dummy.iloc[house_train.shape[0]:].values\n",
    "y_train = house_train.SalePrice.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Outliers Detection and removal '''\n",
    "q1 = train_data_dummy.quantile(0.25)\n",
    "q3 = train_data_dummy.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "train_data_final = train_data_dummy[((train_data_dummy >= (q1 - 1.5*iqr)) & (train_data_dummy <= (q3 + 1.5*iqr))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_final.shape)\n",
    "\n",
    "train_data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating matrices for sklearn:\n",
    "x_train = train_data_final.iloc[:house_train.shape[0]].values\n",
    "x_test = train_data_final.iloc[house_train.shape[0]:].values\n",
    "y_train = house_train.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(house_train.SalePrice))\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_train, y_pred):\n",
    "     return np.sqrt(metrics.mean_squared_error(y_train, y_pred))\n",
    "\n",
    "K = 10    \n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = make_pipeline(RobustScaler(),RidgeCV(alphas=np.arange(14.5, 15.6, 0.1), cv=kf))\n",
    "\n",
    "ridge.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(),LassoCV(alphas=np.arange(0.0001, 0.0009, 0.0001), random_state=42, cv=kf))\n",
    "\n",
    "lasso.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(alphas=np.arange(0.0001, 0.0008, 0.0001),\n",
    "                                                        l1_ratio=np.arange(0.8, 1, 0.025), cv=kf))\n",
    "\n",
    "elasticnet.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =42)\n",
    "\n",
    "\n",
    "GBoost.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor(learning_rate=0.01,\n",
    "                    n_estimators=3500,\n",
    "                    max_depth=3,\n",
    "                    gamma=0.001,\n",
    "                    subsample=0.7,\n",
    "                    colsample_bytree=0.7,\n",
    "                    objective='reg:squarederror',\n",
    "                    nthread=-1,\n",
    "                    seed=42,\n",
    "                    reg_alpha=0.0001)\n",
    "\n",
    "xgb_regressor.fit(x_train,y_train)\n",
    "\n",
    "# y_train_pred_xgb = xgb_regressor.predict(x_train)\n",
    "\n",
    "# print(\"RMSLE score for XGB :\",(np.sqrt(metrics.mean_squared_log_error(y_train,y_train_pred_xgb))))\n",
    "\n",
    "# xgb_preds = xgb_regressor.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' When we have normalised the target value as well ,perform this step '''\n",
    "xgb_preds_final = np.expm1(xgb_preds)\n",
    "print(xgb_preds_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmr = lgb.LGBMRegressor(objective='regression', \n",
    "                      num_leaves=4,\n",
    "                      learning_rate=0.01, \n",
    "                      n_estimators=5000,\n",
    "                      max_bin=200, \n",
    "                      bagging_fraction=0.75,\n",
    "                      bagging_freq=5, \n",
    "                      bagging_seed=42,\n",
    "                      feature_fraction=0.2,\n",
    "                      feature_fraction_seed=42,\n",
    "                      verbose=0)\n",
    "\n",
    "lgbmr.fit(x_train,y_train)\n",
    "\n",
    "# y_actual_pred = lgbmr.predict(x_train)\n",
    "# print(\"RMSLE score for LGBM :\",(np.sqrt(metrics.mean_squared_log_error(y_train,y_actual_pred))))\n",
    "\n",
    "# y_preds_lgbm = lgbmr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = StackingCVRegressor(regressors=(ridge, lasso,elasticnet, GBoost,xgb_regressor, lgbmr), meta_regressor=xgb_regressor,\n",
    "                            use_features_in_secondary=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.fit(x_train,y_train)\n",
    "\n",
    "# y_preds_stack = stack.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_predict(X):\n",
    "    return ((0.05 * lasso.predict(X)) +\n",
    "            (0.1 * ridge.predict(X)) +\n",
    "            (0.1 * elasticnet.predict(X)) +\n",
    "            (0.1 * GBoost.predict(X)) +\n",
    "            (0.25 * xgb_regressor.predict(X)) +\n",
    "            (0.15 * lgbmr.predict(X)) +\n",
    "            (0.3 * stack.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score_comb = rmse(y_train, comb_predict(x_train))\n",
    "print(training_score_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' When we have normalised the target value as well ,perform this step '''\n",
    "y_preds_comb = blend_predict(x_test)\n",
    "y_preds_final = np.expm1(y_preds_comb)\n",
    "print(y_preds_final[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"id\":house_test.Id, \"SalePrice\":y_preds_final})\n",
    "solution.to_csv(\"sachin_solution.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
