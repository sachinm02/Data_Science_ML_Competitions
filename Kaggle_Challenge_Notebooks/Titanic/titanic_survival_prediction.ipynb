{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Various Imports'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV,StratifiedKFold,GridSearchCV,KFold\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Importing Datasets'''\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "train_data = train_data[['PassengerId', 'Name', 'Sex', 'Age', 'Pclass' ,'Cabin', 'Ticket', 'Fare','SibSp',\n",
    "       'Parch', 'Embarked', 'Survived']]\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "test_data = test_data[['PassengerId', 'Name', 'Sex', 'Age', 'Pclass' ,'Cabin', 'Ticket', 'Fare','SibSp',\n",
    "       'Parch', 'Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(len(train_data),len(test_data))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Combining train and test data sets for data processing steps '''\n",
    "\n",
    "titanic_data = pd.concat((train_data.loc[:,'Sex':'Embarked'],test_data.loc[:,'Sex':'Embarked']))\n",
    "titanic_data.drop(columns=['Ticket','Cabin'],inplace=True)\n",
    "print(titanic_data.shape)\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique values in each column\")\n",
    "{c:titanic_data[c].nunique() for c in titanic_data.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of missing values in each column\")\n",
    "{c:100*titanic_data[c].isnull().sum()/len(titanic_data) for c in titanic_data.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Imputing missing values '''\n",
    "imp = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "imp = imp.fit(titanic_data.iloc[:,[1,3]])\n",
    "titanic_data.iloc[:,[1,3]] = imp.transform(titanic_data.iloc[:,[1,3]])\n",
    "print(titanic_data[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Splitting Age feature values into different groups '''\n",
    "bins = [0,16,32,48,64,100]\n",
    "labels = [1,2,3,4,5]\n",
    "\n",
    "titanic_data['age_group'] = pd.cut(titanic_data.Age,bins,labels=labels).astype('int64')\n",
    "print(titanic_data.age_group.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' creating new features '''\n",
    "titanic_data['family_size'] = titanic_data.SibSp + titanic_data.Parch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data['solo'] = 0\n",
    "titanic_data.loc[titanic_data.family_size ==1,'solo'] =1\n",
    "print(titanic_data.solo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Filling Null values '''\n",
    "titanic_data.Embarked = titanic_data.Embarked.fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic_data.drop(columns='Age',inplace=True)\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "corr = titanic_data.corr()\n",
    "high_corr = (corr >= 0.80).astype('uint8')\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(corr,cmap='RdBu_r',annot=True,center=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of missing values in each column :\")\n",
    "{c:100*titanic_data[c].isnull().sum()/len(titanic_data) for c in titanic_data.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Checking for skewness in features '''\n",
    "numeric_features = titanic_data.dtypes[titanic_data.dtypes != 'object'].index\n",
    "skewness = pd.DataFrame({'skewness':titanic_data[numeric_features].apply(lambda x : stats.skew(x.dropna()))})\n",
    "skewness = skewness.sort_values('skewness',ascending=False)\n",
    "skewness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating new dataframe with dummy vars and required features only '''\n",
    "titanic_data_dummy = pd.get_dummies(titanic_data.drop(columns='Age'),columns=['Sex','Embarked'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_dummy.info()\n",
    "print(titanic_data_dummy.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Handling of skewed data  '''\n",
    "titanic_data_dummy = np.log1p(titanic_data_dummy)\n",
    "titanic_data_dummy.drop(columns=['SibSp','Parch'],axis=1,inplace=True)\n",
    "print(titanic_data_dummy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.distplot(train_data.Survived,fit=stats.norm)\n",
    "qqplot(train_data.Survived)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(titanic_data_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating train and test data sets '''\n",
    "x_train = titanic_data_dummy.iloc[:train_data.shape[0],:].values\n",
    "x_test =titanic_data_dummy.iloc[train_data.shape[0]:,:].values\n",
    "y_train = train_data.Survived.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train),len(y_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here ,paramter values have been updated after performing Randomized search for finding best estimators\n",
    "classifier_rf = RandomForestClassifier(criterion='gini', \n",
    "                                           n_estimators=1100,\n",
    "                                           max_depth=5,\n",
    "                                           min_samples_split=4,\n",
    "                                           min_samples_leaf=5,\n",
    "                                           max_features='auto',\n",
    "                                           oob_score=True,\n",
    "                                           random_state=42,\n",
    "                                           n_jobs=-1,\n",
    "                                           verbose=1)\n",
    "classifier_rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':[100,300,500,1000],'max_features':['auto','sqrt','log2'],'max_depth':[2,4,6,8],\n",
    "               'min_samples_leaf':[1,2,3,4],'criterion':['gini','entropy']}\n",
    "cv = KFold(n_splits=10,shuffle=True,random_state=42)\n",
    "param_search = RandomizedSearchCV(estimator=classifier_rf,param_distributions=param_grid,cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_search.best_estimator_)\n",
    "param_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = classifier_rf.predict(x_train)\n",
    "y_test_pred = classifier_rf.predict(x_test)\n",
    "print(y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gbc = GradientBoostingClassifier(n_estimators=240,max_depth=4,learning_rate=0.08,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_NB = GaussianNB()\n",
    "# classifier_NB.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = classifier_NB.predict(x_train)\n",
    "y_test_pred = classifier_NB.predict(x_test)\n",
    "print(y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_xgb = XGBClassifier(max_depth=3,\n",
    "                                learning_rate=0.1,\n",
    "                                n_estimators=3000,\n",
    "                                objective='binary:logistic',\n",
    "                                random_state=42)\n",
    "# classifier_xgb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_lgbm = LGBMClassifier(boosting_type='gbdt',\n",
    "                                    learning_rate=0.1,\n",
    "                                    n_estimators=1000,\n",
    "                                    objective='binary',\n",
    "                                    random_state=42)\n",
    "# classifier_lgbm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_classifier = VotingClassifier(estimators=[('rf',classifier_rf),('gbc',classifier_gbc),\n",
    "                                                ('gnb',classifier_NB),('xgb',classifier_xgb),('lgbm',classifier_lgbm)],\n",
    "                                    voting='hard')\n",
    "stack_classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = stack_classifier.predict(x_train)\n",
    "y_test_pred = stack_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the Neural Network\n",
    "model = Sequential()\n",
    "\n",
    "# layers\n",
    "model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 10))\n",
    "model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 3, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Dense()\n",
    "# Train the ANN\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Predicting results '''\n",
    "y_test_nn = model.predict(x_test)\n",
    "\n",
    "y_test_pred = (y_test_nn>0.50).astype('int8').reshape(x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Checking accuracy '''\n",
    "cm = metrics.confusion_matrix(y_train,y_train_pred)\n",
    "print(cm)\n",
    "print((cm[0][0]+cm[1][1])/cm.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy score :',metrics.accuracy_score(y_train,y_train_pred))\n",
    "print('Precision score :',metrics.precision_score(y_train,y_train_pred))\n",
    "print('Recall score :',metrics.recall_score(y_train,y_train_pred))\n",
    "print('F1 score :',metrics.f1_score(y_train,y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Exporting results for submission'''\n",
    "solution = pd.DataFrame({\"PassengerId\":test_data.PassengerId, \"Survived\":y_test_pred})\n",
    "solution.to_csv(\"submission_files/sachin_solution_nn.csv\", index = False)\n",
    "print(solution.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
